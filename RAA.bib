@article{replicationvsreproductiblity,
author = {Drummond, Chris},
year = {2009},
month = {01},
pages = {},
title = {Replicability Is Not Reproducibility: Nor Is It Good Science},
journal = {Proceedings of the Evaluation Methods for Machine Learning Workshop at the 26th ICML}
}

@misc{pineau2020improving,
	title={Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)},
	author={Joelle Pineau and Philippe Vincent-Lamarre and Koustuv Sinha and Vincent Larivière and Alina Beygelzimer and Florence d'Alché-Buc and Emily Fox and Hugo Larochelle},
	year={2020},
	eprint={2003.12206},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@article{baker500ScientistsLift2016,
	title = {1,500 Scientists Lift the Lid on Reproducibility},
	author = {Baker, Monya},
	year = {2016},
	month = may,
	volume = {533},
	pages = {452},
	doi = {10.1038/533452a},
	chapter = {News Feature},
	journal = {Nature News},
	number = {7604},
	url={https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970}
}

@misc{raff2019step,
	title={A Step Toward Quantifying Independently Reproducible Machine Learning Research},
	author={Edward Raff},
	year={2019},
	eprint={1909.06674},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@incollection{NEURIPS2019_9015,
	title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	booktitle = {Advances in Neural Information Processing Systems 32},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages = {8024--8035},
	year = {2019},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@misc{poutyne,
	author = {Paradis, Fr{\'e}d{\'e}rik and Beauchemin, David and Godbout, Mathieu and Alain, Mathieu and Garneau, Nicolas and Otte, Stefan and Tremblay, Alexis and B{\'e}langer, Marc-Antoine and Laviolette, Fran{\c{c}}ois},
	title  = {{Poutyne: A Simplified Framework for Deep Learning}},
	year   = {2020},
	note   = {\url{https://poutyne.org}}
}

@article{falcon2019pytorch,
	title={PyTorch Lightning},
	author={Falcon, WA},
	journal={GitHub. Note: https://github.com/PyTorchLightning/pytorch-lightning},
	volume={3},
	year={2019}
}

@inproceedings{sklearn_api,
	author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
	Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
	Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
	and Jaques Grobler and Robert Layton and Jake VanderPlas and
	Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
	title     = {{API} design for machine learning software: experiences from the scikit-learn
	project},
	booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
	year      = {2013},
	pages = {108--122},
}

@inproceedings{rehurek_lrec,
	title = {{Software Framework for Topic Modelling with Large Corpora}},
	author = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
	booktitle = {{Proceedings of the LREC 2010 Workshop on New
	Challenges for NLP Frameworks}},
	pages = {45--50},
	year = 2010,
	month = May,
	day = 22,
	publisher = {ELRA},
	address = {Valletta, Malta},
	note={\url{http://is.muni.cz/publication/884893/en}},
	language={English}
}

@inproceedings{Gardner2017AllenNLP,
	title={AllenNLP: A Deep Semantic Natural Language Processing Platform},
	author={Matt Gardner and Joel Grus and Mark Neumann and Oyvind Tafjord
	and Pradeep Dasigi and Nelson F. Liu and Matthew Peters and
	Michael Schmitz and Luke S. Zettlemoyer},
	year={2017},
	Eprint = {arXiv:1803.07640},
}

@article{Zaharia2018AcceleratingTM,
	title={Accelerating the Machine Learning Lifecycle with MLflow},
	author={M. Zaharia and Andrew Chen and A. Davidson and A. Ghodsi and S. Hong and A. Konwinski and Siddharth Murching and Tomas Nykodym and P. Ogilvie and Mani Parkhe and F. Xie and Corey Zumar},
	journal={IEEE Data Eng. Bull.},
	year={2018},
	volume={41},
	pages={39-45}
}

@inproceedings{sacred,
	author = {Greff, Klaus and Klein, Aaron and Chovanec, Martin and Hutter, Frank and Schmidhuber, Jürgen},
	year = {2017},
	month = {01},
	pages = {49-56},
	title = {The Sacred Infrastructure for Computational Research},
	doi = {10.25080/shinma-7f4c6e7-008}
}

@misc{garneau2020robust,
	title={A Robust Self-Learning Method for Fully Unsupervised Cross-Lingual Mappings of Word Embeddings: Making the Method Robustly Reproducible as Well}, 
	author={Nicolas Garneau and Mathieu Godbout and David Beauchemin and Audrey Durand and Luc Lamontagne},
	year={2020},
	eprint={1912.01706},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@inproceedings{gardner-etal-2018-writing,
	title = "Writing Code for {NLP} Research",
	author = "Gardner, Matt  and
	Neumann, Mark  and
	Grus, Joel  and
	Lourie, Nicholas",
	booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts",
	month = oct # "-" # nov,
	year = "2018",
	address = "Melbourne, Australia",
	publisher = "Association for Computational Linguistics",
	abstract = "Doing modern NLP research requires writing code. Good code enables fast prototyping, easy debugging, controlled experiments, and accessible visualizations that help researchers understand what a model is doing. Bad code leads to research that is at best hard to reproduce and extend, and at worst simply incorrect. Indeed, there is a growing recognition of the importance of having good tools to assist good research in our field, as the upcoming workshop on open source software for NLP demonstrates. This tutorial aims to share best practices for writing code for NLP research, drawing on the instructors' experience designing the recently-released AllenNLP toolkit, a PyTorch-based library for deep learning NLP research. We will explain how a library with the right abstractions and components enables better code and better science, using models implemented in AllenNLP as examples. Participants will learn how to write research code in a way that facilitates good science and easy experimentation, regardless of what framework they use.",
}